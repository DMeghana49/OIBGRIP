{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea475c55",
   "metadata": {},
   "source": [
    "# EMAIL SPAM DETECTION WITH MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda5515",
   "metadata": {},
   "source": [
    "We have all been the recipient of spam mails before. Spam mail,or junk mail, is a type of email that is sent to a massive number of users at one time, frequently containing cryptic messages, scams ,or most dangerously, phishing content.\n",
    "        In this project , we use Python to build an email spam detector. Then use machine learning to train the spam detector to recognize and classify emails into spam and non-spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac19cfd",
   "metadata": {},
   "source": [
    "**Modules needed:\n",
    "\n",
    "pandas: Pandas is an opensource library that allows you to perform data manipulation in Python. Pandas provide an easy way to create, manipulate and wrangle the data.                                                                             \n",
    "\n",
    "\n",
    "numpy: Numpy is the fundamental package for scientific computing with Python. numpy can be used as an efficient multi-dimensional container of generic data.                                                                                \n",
    "\n",
    "matplotlib: Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of formats.         \n",
    "\n",
    "seaborn: Seaborn is a Python data-visualization library that is based on matplotlib. Seaborn provides a high-level interface for drawing attractive and informative statistical graphics.                                                   \n",
    "\n",
    "scipy: Scipy is a Python-based ecosystem of open-source software for mathematics, science, and engineering.                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f326f178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\meghana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\meghana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\meghana\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27fb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading of csv file\n",
    "df = pd.read_csv('email_spam_detection.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "158f4236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email spam detection dataset is: \n",
      "         v1                                                 v2 Unnamed: 2  \\\n",
      "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "...    ...                                                ...        ...   \n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
      "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
      "5571   ham                         Rofl. Its true to its name        NaN   \n",
      "\n",
      "     Unnamed: 3 Unnamed: 4  \n",
      "0           NaN        NaN  \n",
      "1           NaN        NaN  \n",
      "2           NaN        NaN  \n",
      "3           NaN        NaN  \n",
      "4           NaN        NaN  \n",
      "...         ...        ...  \n",
      "5567        NaN        NaN  \n",
      "5568        NaN        NaN  \n",
      "5569        NaN        NaN  \n",
      "5570        NaN        NaN  \n",
      "5571        NaN        NaN  \n",
      "\n",
      "[5572 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Displaying dataset\n",
    "print(\"Email spam detection dataset is: \\n\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e319c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows of the dataset are: \n",
      "      v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "\n",
      "Bottom 5 rows of the dataset are: \n",
      "         v1                                                 v2 Unnamed: 2  \\\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
      "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
      "5571   ham                         Rofl. Its true to its name        NaN   \n",
      "\n",
      "     Unnamed: 3 Unnamed: 4  \n",
      "5567        NaN        NaN  \n",
      "5568        NaN        NaN  \n",
      "5569        NaN        NaN  \n",
      "5570        NaN        NaN  \n",
      "5571        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "#Top and Botton 5 rows of car price prediction dataset\n",
    "print(\"Top 5 rows of the dataset are: \\n\",df.head())\n",
    "print(\"\\n\\nBottom 5 rows of the dataset are: \\n\",df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2bce7",
   "metadata": {},
   "source": [
    "The Porter stemming algorithm (or ‘Porter stemmer’) is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae884da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "lemmatize=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "for i in range(0,len(df)):\n",
    "  review=re.sub('[^a-zA-Z]', ' ', df['v2'][i])\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "    \n",
    "  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43ccdc",
   "metadata": {},
   "source": [
    "CountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6023c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y=pd.get_dummies(df['v1'])\n",
    "y=y.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d80d6",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a52fbfd",
   "metadata": {},
   "source": [
    "With train_test_split (), you need to provide the sequences that you want to split as well as any optional arguments. It returns a list of NumPy arrays, other sequences, or SciPy sparse matrices if appropriate: arrays is the sequence of lists, NumPy arrays, pandas DataFrames, or similar array-like objects that hold the data you want to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0efe08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072165a",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier separates data into different classes according to the Bayes’ Theorem, along with the assumption that all the predictors are independent of one another. It assumes that a particular feature in a class is not related to the presence of other features.\n",
    "\n",
    "\n",
    "\n",
    "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22d8b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9e43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=spam_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd7540",
   "metadata": {},
   "source": [
    "**Model precision:                                                                                                              \n",
    "\n",
    "The model precision score measures the proportion of positively predicted labels that are actually correct. Precision is also known as the positive predictive value. Precision is used in conjunction with the recall to trade-off false positives and false negatives. Precision is affected by the class distribution. \n",
    "                        **Precision Score = TP / (FP + TP)\n",
    "\n",
    "\n",
    "**Accuracy Score:                                                                                                                  \n",
    "\n",
    "Model recall score represents the model’s ability to correctly predict the positives out of actual positives. This is unlike precision which measures how many predictions made by models are actually positive out of all positive predictions made. \n",
    "                        **Recall Score = TP / (FN + TP)\n",
    "                        \n",
    "**Confusion Matrix:                                                                                                              \n",
    " \n",
    " A confusion matrix, also referred to as an error matrix, is a process that helps to assess and predict the validity of a classification model. Using confusion matrices allows you to see different errors which you could make when you make predictions.\n",
    "\n",
    "**Classification Report                                                                                                         \n",
    "\n",
    "A classification report is a performance evaluation metric in machine learning. It is used to show the precision, recall, F1 Score, and support of your trained classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98f4e18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[943   6]\n",
      " [  9 157]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacd3f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.9865470852017937\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd6d301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       949\n",
      "           1       0.96      0.95      0.95       166\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.97      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report: {}\".format(classification_report(y_test,y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
